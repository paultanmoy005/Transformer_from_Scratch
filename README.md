# Transformer_from_Scratch
Here a basic Transformer model is built from scratch using PyTorch. The Transformer model, introduced by Vaswani et al. in the paper “Attention is All You Need,” is a deep learning architecture designed for sequence-to-sequence tasks, such as machine translation and text summarization.

Here we have built the basic buidling blocks of a transformer:

- Multi-Head Attention
- Position-wise Feed-Forward Networks
- Positional Encoding
- Build the Encoder and Decoder layers
- Combine Encoder and Decoder layers to create the complete Transformer model

In this demo project, a toy dataset is created for demonstration purposes. In practice, we would use a larger dataset, preprocess the text, and create vocabulary mappings for source and target languages.

Modify the main.py accordingly to generate the sample data, build the transformer model, and train the model.
